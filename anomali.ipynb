{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8e90ef-a215-4165-90cc-c494de34d51f",
   "metadata": {},
   "source": [
    "# Network Anomaly Detection\n",
    "Random forests, which are ensembles of decision trees, effectively handle complex, high-dimensional data and can be used to detect these anomalous patterns.\n",
    "When used for anomaly detection, a random forest is trained exclusively on data representing normal conditions.\n",
    "# NSL-KDD Dataset\n",
    "\n",
    "The NSL-KDD dataset refines the original KDD Cup 1999 dataset by eliminating redundant entries and correcting imbalanced class distributions. Researchers commonly adopt it as a standard reference for measuring the performance of various intrusion detection models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac9759d6-0d49-45ae-854d-5ba3f76d275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline # this one support SMOT\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de14858-b7e0-400d-aa41-cfd0613184a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>attack</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>SF</td>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>udp</td>\n",
       "      <td>other</td>\n",
       "      <td>SF</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neptune</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration protocol_type   service flag  src_bytes  dst_bytes  land  \\\n",
       "0         0           tcp  ftp_data   SF        491          0     0   \n",
       "1         0           udp     other   SF        146          0     0   \n",
       "2         0           tcp   private   S0          0          0     0   \n",
       "\n",
       "   wrong_fragment  urgent  hot  ...  dst_host_same_srv_rate  \\\n",
       "0               0       0    0  ...                    0.17   \n",
       "1               0       0    0  ...                    0.00   \n",
       "2               0       0    0  ...                    0.10   \n",
       "\n",
       "   dst_host_diff_srv_rate  dst_host_same_src_port_rate  \\\n",
       "0                    0.03                         0.17   \n",
       "1                    0.60                         0.88   \n",
       "2                    0.05                         0.00   \n",
       "\n",
       "   dst_host_srv_diff_host_rate  dst_host_serror_rate  \\\n",
       "0                          0.0                   0.0   \n",
       "1                          0.0                   0.0   \n",
       "2                          0.0                   1.0   \n",
       "\n",
       "   dst_host_srv_serror_rate  dst_host_rerror_rate  dst_host_srv_rerror_rate  \\\n",
       "0                       0.0                  0.05                       0.0   \n",
       "1                       0.0                  0.00                       0.0   \n",
       "2                       1.0                  0.00                       0.0   \n",
       "\n",
       "    attack  level  \n",
       "0   normal     20  \n",
       "1   normal     15  \n",
       "2  neptune     19  \n",
       "\n",
       "[3 rows x 43 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the file path to the dataset\n",
    "file_path = r'KDD+.txt'\n",
    "\n",
    "# What is inside KDD+.txt:\n",
    "# 0,tcp,ftp_data,SF,491,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0.0,0.0,0.0,0.0,1.0,0.0,0.0,150,25,0.17,0.03,0.17,0.0,0.0,0.0,0.05,0.0,normal,20\n",
    "# 0,udp,other,SF,146,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,13,1,0.0,0.0,0.0,0.0,0.08,0.15,0.0,255,1,0.0,0.6,0.88,0.0,0.0,0.0,0.0,0.0,normal,15\n",
    "# 0,tcp,private,S0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,123,6,1.0,1.0,0.0,0.0,0.05,0.07,0.0,255,26,0.1,0.05,0.0,0.0,1.0,1.0,0.0,0.0,neptune,19\n",
    "\n",
    "# Define the column names corresponding to the NSL-KDD dataset\n",
    "columns = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', \n",
    "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', \n",
    "    'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', \n",
    "    'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', \n",
    "    'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', \n",
    "    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', \n",
    "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', \n",
    "    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', \n",
    "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'attack', 'level'\n",
    "]\n",
    "# Read the combined NSL-KDD dataset into a DataFrame\n",
    "df = pd.read_csv(file_path, names=columns)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f394f70-88aa-4330-9b23-74e8794525a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see the attack column having different values: normal and others.\n",
    "# create a new column and add 0 for normal traffic and 1 for the rest\n",
    "df['attack_flag'] = df['attack'].apply(lambda a: 0 if a == 'normal' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f6abae-2918-4897-b9a3-d50137b4e8b5",
   "metadata": {},
   "source": [
    "Define lists categorizing specific attacks into four major groups:\n",
    "\n",
    "    DoS (Denial of Service) attacks such as neptune and smurf\n",
    "    Probe attacks that scan networks for vulnerabilities, like satan or ipsweep\n",
    "    Privilege Escalation attacks that attempt to gain unauthorized admin-level control, such as buffer_overflow\n",
    "    Access attacks that seek to breach system access controls, like guess_passwd\n",
    "\n",
    "\n",
    "    0 for normal traffic\n",
    "    1 for DoS attacks\n",
    "    2 for Probe attacks\n",
    "    3 for Privilege Escalation attacks\n",
    "    4 for Access attacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c6277f-3ecc-4540-890d-0bfbaf8f0470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class classification target categories\n",
    "dos_attacks = ['apache2', 'back', 'land', 'neptune', 'mailbomb', 'pod', \n",
    "               'processtable', 'smurf', 'teardrop', 'udpstorm', 'worm']\n",
    "probe_attacks = ['ipsweep', 'mscan', 'nmap', 'portsweep', 'saint', 'satan']\n",
    "privilege_attacks = ['buffer_overflow', 'loadmdoule', 'perl', 'ps', \n",
    "                     'rootkit', 'sqlattack', 'xterm']\n",
    "access_attacks = ['ftp_write', 'guess_passwd', 'http_tunnel', 'imap', \n",
    "                  'multihop', 'named', 'phf', 'sendmail', 'snmpgetattack', \n",
    "                  'snmpguess', 'spy', 'warezclient', 'warezmaster', \n",
    "                  'xclock', 'xsnoop']\n",
    "def map_attack(attack):\n",
    "    if attack in dos_attacks:\n",
    "        return 1\n",
    "    elif attack in probe_attacks:\n",
    "        return 2\n",
    "    elif attack in privilege_attacks:\n",
    "        return 3\n",
    "    elif attack in access_attacks:\n",
    "        return 4\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Assign multi-class category to each row\n",
    "df['attack_map'] = df['attack'].apply(map_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e01e0a-f030-44b7-bed0-a7338a1f41fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Categorical Variables\n",
    "# protocol_type (e.g., tcp, udp) and service (e.g., http, ftp)\n",
    "# Encoding categorical variables\n",
    "features_to_encode = ['protocol_type', 'service']\n",
    "encoded = pd.get_dummies(df[features_to_encode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d80c18d6-0a63-43ba-ae68-d536dd507a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Numeric Features\n",
    "# Beyond categorical variables, the dataset contains a range of numeric features that describe various aspects of network traffic. \n",
    "#These include basic metrics like duration, src_bytes, and dst_bytes, as well as more specialized features such as serror_rate and dst_host_srv_diff_host_rate, \n",
    "# which capture statistical properties of the network sessions. \n",
    "# Numeric features that capture various statistical properties of the traffic\n",
    "numeric_features = [\n",
    "    'duration', 'src_bytes', 'dst_bytes', 'wrong_fragment', 'urgent', 'hot', \n",
    "    'num_failed_logins', 'num_compromised', 'root_shell', 'su_attempted', \n",
    "    'num_root', 'num_file_creations', 'num_shells', 'num_access_files', \n",
    "    'num_outbound_cmds', 'count', 'srv_count', 'serror_rate', \n",
    "    'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', \n",
    "    'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', \n",
    "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', \n",
    "    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', \n",
    "    'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', \n",
    "    'dst_host_srv_rerror_rate'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7edac6f-ada0-460b-89e1-d0e35d6377a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the one-hot encoded categorical features with the selected numeric features. We join them into a single DataFrame train_set\n",
    "# Combine encoded categorical variables and numeric features\n",
    "train_set = encoded.join(df[numeric_features])\n",
    "# target variable\n",
    "multi_y = df['attack_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fbbde4d-7316-4724-b254-834accc7cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets (80% pentru antrenare (train_X È™i train_y) 20% pentru testare (test_X È™i test_y))\n",
    "train_X, test_X, train_y, test_y = train_test_split(train_set, multi_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2412ec3-7bc6-494a-8cb3-8b48fe9bbd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further split the training set into separate training and validation sets\n",
    "# This supports model tuning and hyperparameter optimization without contaminating the final test data.\n",
    "\n",
    "multi_train_X, multi_val_X, multi_train_y, multi_val_y = train_test_split(train_X, train_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3375ab79-8b85-4228-a35d-6279ed7911bb",
   "metadata": {},
   "source": [
    "After splitting, we have:\n",
    "\n",
    "    train_X, train_y: Core training set\n",
    "    test_X, test_y: Reserved for the final performance evaluation\n",
    "    multi_train_X, multi_train_y: Training subset for fitting the model\n",
    "    multi_val_X, multi_val_y: Validation subset for hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd90e147-7772-49c2-a351-a4619db4495b",
   "metadata": {},
   "source": [
    "TOATE DATELE\n",
    "\n",
    " â””â”€â”€ 80% Train/Validation\n",
    " \n",
    "     â”œâ”€â”€ 70% Train (multi_train_X, multi_train_y)\n",
    "     â””â”€â”€ 30% Validation (multi_val_X, multi_val_y)\n",
    " \n",
    " â””â”€â”€ 20% Test (test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7ad6375-813d-4d63-8588-039c49165529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistribuÈ›ia claselor Ã®nainte de SMOTE:\n",
      "[43032 30057  7933    59  2088]\n"
     ]
    }
   ],
   "source": [
    "###################### if using SMOTE only on privilege class prepare before ###################\n",
    "# VerificÄƒm distribuÈ›ia claselor Ã®nainte de a aplica SMOTE\n",
    "print(\"DistribuÈ›ia claselor Ã®nainte de SMOTE:\")\n",
    "print(np.bincount(multi_train_y))\n",
    "# clasa 3 (Privilege) conÈ›ine doar 59 de exemple, ceea ce este prea puÈ›in pentru a aplica SMOTE eficient.\n",
    "# OPTIONS TO FIX THIS:\n",
    "# 1) add more privesc data\n",
    "# 2) add smote only on the other classes except privesc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d17fc90a-239c-451f-99d5-9739400f28a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters: {}\n",
      "Best Score (on train+val split): 0.9954592645095399\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# ðŸ”¥ PIPELINE + SMOTE + RANDOM FOREST + GRID SEARCH ðŸ”¥\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    #('smote', SMOTE(random_state=42)),\n",
    "    #('rf', RandomForestClassifier(n_estimators=200,random_state=42)), # with SMOTE\n",
    "    #('rf', RandomForestClassifier(n_estimators=200, class_weight='balanced',random_state=42)) # without SMOTE but with class_weight='balanced'\n",
    "    ('xgb', XGBClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for RandomForest\n",
    "param_grid = {\n",
    "    #'rf__n_estimators': [100, 200],\n",
    "    #'rf__max_depth': [10, 20, None],\n",
    "    #'rf__min_samples_split': [2, 5]\n",
    "    #'xgb__n_estimators': [100, 200, 300],  # NumÄƒrul de estimatori\n",
    "    #'xgb__learning_rate': [0.01, 0.1, 0.2],  # Rata de Ã®nvÄƒÈ›are\n",
    "    #'xgb__max_depth': [3, 6, 9],  # AdÃ¢ncimea maximÄƒ a arborilor\n",
    "    #'xgb__subsample': [0.8, 1.0],  # ProporÈ›ia de date folosite la antrenament\n",
    "    #'xgb__colsample_bytree': [0.8, 1.0]  # ProporÈ›ia de trÄƒsÄƒturi folosite pe fiecare arbore\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Define GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, \n",
    "    param_grid, \n",
    "    cv=3, \n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=2 # use 2 cores\n",
    "    #verbose=2,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "grid_search.fit(multi_train_X, multi_train_y)\n",
    "\n",
    "# Best model results\n",
    "print(\"\\nBest Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score (on train+val split):\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10bbaacd-9cc3-4ee9-b683-b88471ed027d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Set Evaluation:\n",
      "Accuracy: 0.9964\n",
      "Precision: 0.9964\n",
      "Recall: 0.9964\n",
      "F1-Score: 0.9964\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# ðŸ”¥ EVALUARE PE VALIDATION ðŸ”¥\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "multi_predictions = grid_search.best_estimator_.predict(multi_val_X)\n",
    "\n",
    "accuracy = accuracy_score(multi_val_y, multi_predictions)\n",
    "precision = precision_score(multi_val_y, multi_predictions, average='weighted')\n",
    "recall = recall_score(multi_val_y, multi_predictions, average='weighted')\n",
    "f1 = f1_score(multi_val_y, multi_predictions, average='weighted')\n",
    "\n",
    "print(\"\\nValidation Set Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "013978b8-6f3c-4ad8-94d2-e98f8642f6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report for Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       1.00      1.00      1.00     18689\n",
      "         DoS       1.00      1.00      1.00     12642\n",
      "       Probe       1.00      1.00      1.00      3395\n",
      "   Privilege       0.76      0.50      0.60        26\n",
      "      Access       0.99      0.93      0.96       892\n",
      "\n",
      "    accuracy                           1.00     35644\n",
      "   macro avg       0.95      0.89      0.91     35644\n",
      "weighted avg       1.00      1.00      1.00     35644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report for Validation Set\n",
    "print(\"\\nClassification Report for Validation Set:\")\n",
    "class_labels = ['Normal', 'DoS', 'Probe', 'Privilege', 'Access']\n",
    "print(classification_report(multi_val_y, multi_predictions, target_names=class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91775cd7-6144-4e19-9080-2f9baae8e086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation:\n",
      "Accuracy: 0.9957\n",
      "Precision: 0.9956\n",
      "Recall: 0.9957\n",
      "F1-Score: 0.9956\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# ðŸ”¥ EVALUARE PE TEST ðŸ”¥\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "test_multi_predictions = grid_search.best_estimator_.predict(test_X)\n",
    "\n",
    "test_accuracy = accuracy_score(test_y, test_multi_predictions)\n",
    "test_precision = precision_score(test_y, test_multi_predictions, average='weighted')\n",
    "test_recall = recall_score(test_y, test_multi_predictions, average='weighted')\n",
    "test_f1 = f1_score(test_y, test_multi_predictions, average='weighted')\n",
    "\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1-Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62afe896-4145-4248-abf1-fe974445ff13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report for Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       1.00      1.00      1.00     15486\n",
      "         DoS       1.00      1.00      1.00     10688\n",
      "       Probe       0.99      1.00      0.99      2749\n",
      "   Privilege       0.82      0.61      0.70        23\n",
      "      Access       0.98      0.93      0.95       758\n",
      "\n",
      "    accuracy                           1.00     29704\n",
      "   macro avg       0.96      0.91      0.93     29704\n",
      "weighted avg       1.00      1.00      1.00     29704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report for Test Set\n",
    "print(\"\\nClassification Report for Test Set:\")\n",
    "print(classification_report(test_y, test_multi_predictions, target_names=class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4d6a9d-ee64-4049-a63a-c5af1515e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## RANDOM FOREST SIMPLE ######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe0de47f-b593-4282-b2b6-76815c5901f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# RANDOM FOREST simple ðŸ”¥\n",
    "# ------------------------------------------------------------------------\n",
    "#######class_weight='balanced' => take more serious rare classes like privilege escalation\n",
    "\n",
    "#rf_model_multi = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "#rf_model_multi.fit(multi_train_X, multi_train_y)\n",
    "\n",
    "###### Predict and evaluate the model on the validation set#######\n",
    "#multi_predictions = rf_model_multi.predict(multi_val_X)\n",
    "#accuracy = accuracy_score(multi_val_y, multi_predictions)\n",
    "#precision = precision_score(multi_val_y, multi_predictions, average='weighted')\n",
    "#recall = recall_score(multi_val_y, multi_predictions, average='weighted')\n",
    "#f1 = f1_score(multi_val_y, multi_predictions, average='weighted')\n",
    "#print(f\"Validation Set Evaluation:\")\n",
    "#print(f\"Accuracy: {accuracy:.4f}\")\n",
    "#print(f\"Precision: {precision:.4f}\")\n",
    "#print(f\"Recall: {recall:.4f}\")\n",
    "#print(f\"F1-Score: {f1:.4f}\")\n",
    "##### Classification Report for Validation Set#####\n",
    "#print(\"Classification Report for Validation Set:\")\n",
    "#class_labels = ['Normal', 'DoS', 'Probe', 'Privilege', 'Access']\n",
    "#print(classification_report(multi_val_y, multi_predictions, target_names=class_labels))\n",
    "\n",
    "###### Final evaluation on the test set #####\n",
    "#test_multi_predictions = rf_model_multi.predict(test_X)\n",
    "#test_accuracy = accuracy_score(test_y, test_multi_predictions)\n",
    "#test_precision = precision_score(test_y, test_multi_predictions, average='weighted')\n",
    "#test_recall = recall_score(test_y, test_multi_predictions, average='weighted')\n",
    "#test_f1 = f1_score(test_y, test_multi_predictions, average='weighted')\n",
    "#print(\"\\nTest Set Evaluation:\")\n",
    "#print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "#print(f\"Precision: {test_precision:.4f}\")\n",
    "#print(f\"Recall: {test_recall:.4f}\")\n",
    "#print(f\"F1-Score: {test_f1:.4f}\")\n",
    "\n",
    "######## Classification Report for Test Set #############\n",
    "#print(\"Classification Report for Test Set:\")\n",
    "#print(classification_report(test_y, test_multi_predictions, target_names=class_labels))\n",
    "\n",
    "################ Very bad on privilege escalation because I have only few examples.###################\n",
    "################ Let's use SMOTE to \"invent\" more and balance the categories###########################\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "#smote = SMOTE(random_state=42)\n",
    "#multi_train_X_smote, multi_train_y_smote = smote.fit_resample(multi_train_X, multi_train_y)\n",
    "#print(\"Shape original:\", multi_train_X.shape)\n",
    "#print(\"Shape dupa SMOTE:\", multi_train_X_smote.shape)\n",
    "\n",
    "################ Train on the generated data ##################\n",
    "#rf_model_multi.fit(multi_train_X_smote, multi_train_y_smote)\n",
    "\n",
    "########### Prezici pe validation (care nu a fost modificat!!)#################\n",
    "#multi_predictions = rf_model_multi.predict(multi_val_X)\n",
    "\n",
    "######### Evaluezi #################\n",
    "#print(classification_report(multi_val_y, multi_predictions, target_names=class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea945f09-3275-4a0f-b444-e5f2603ada54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "896c1d34-7cf7-4295-8848-87ce62646a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "06eaa66a-e83c-4f7f-b503-e83163332546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "#model_filename = 'network_anomaly_detection_model.joblib'\n",
    "#joblib.dump(rf_model_multi, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5249e84e-3904-42b9-8b60-f4afa4737998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
